{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB | Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data**\n",
    "\n",
    "In this challenge, we will be working with the same Spaceship Titanic data, like the previous Lab. The data can be found here:\n",
    "\n",
    "https://raw.githubusercontent.com/data-bootcamp-v4/data/main/spaceship_titanic.csv\n",
    "\n",
    "Metadata\n",
    "\n",
    "https://github.com/data-bootcamp-v4/data/blob/main/spaceship_titanic.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Lab, you should try different ensemble methods in order to see if can obtain a better model than before. In order to do a fair comparison, you should perform the same feature scaling, engineering applied in previous Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>Name</th>\n",
       "      <th>Transported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>B/0/P</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>39.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Maham Ofracculy</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>24.0</td>\n",
       "      <td>False</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Juanna Vines</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0003_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>58.0</td>\n",
       "      <td>True</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3576.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6715.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>Altark Susent</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0003_02</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>33.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>3329.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>Solam Susent</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0004_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/1/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>16.0</td>\n",
       "      <td>False</td>\n",
       "      <td>303.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Willy Santantines</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PassengerId HomePlanet CryoSleep  Cabin  Destination   Age    VIP  \\\n",
       "0     0001_01     Europa     False  B/0/P  TRAPPIST-1e  39.0  False   \n",
       "1     0002_01      Earth     False  F/0/S  TRAPPIST-1e  24.0  False   \n",
       "2     0003_01     Europa     False  A/0/S  TRAPPIST-1e  58.0   True   \n",
       "3     0003_02     Europa     False  A/0/S  TRAPPIST-1e  33.0  False   \n",
       "4     0004_01      Earth     False  F/1/S  TRAPPIST-1e  16.0  False   \n",
       "\n",
       "   RoomService  FoodCourt  ShoppingMall     Spa  VRDeck               Name  \\\n",
       "0          0.0        0.0           0.0     0.0     0.0    Maham Ofracculy   \n",
       "1        109.0        9.0          25.0   549.0    44.0       Juanna Vines   \n",
       "2         43.0     3576.0           0.0  6715.0    49.0      Altark Susent   \n",
       "3          0.0     1283.0         371.0  3329.0   193.0       Solam Susent   \n",
       "4        303.0       70.0         151.0   565.0     2.0  Willy Santantines   \n",
       "\n",
       "   Transported  \n",
       "0        False  \n",
       "1         True  \n",
       "2        False  \n",
       "3        False  \n",
       "4         True  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spaceship = pd.read_csv(\"https://raw.githubusercontent.com/data-bootcamp-v4/data/main/spaceship_titanic.csv\")\n",
    "spaceship.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform the same as before:\n",
    "- Feature Scaling\n",
    "- Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features scaled using StandardScaler.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Define features and target\n",
    "features = spaceship.drop(columns=['Transported'])\n",
    "target = spaceship['Transported']\n",
    "\n",
    "# Perform train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train.select_dtypes(include=[np.number]))\n",
    "X_test_imputed = imputer.transform(X_test.select_dtypes(include=[np.number]))\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training features and transform them\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "\n",
    "# Transform the test features using the fitted scaler\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "print(\"Features scaled using StandardScaler.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test sets created and features scaled.\n",
      "X_train_scaled shape: (6954, 23735)\n",
      "X_test_scaled shape: (1739, 23735)\n",
      "y_train shape: (6954,)\n",
      "y_test shape: (1739,)\n"
     ]
    }
   ],
   "source": [
    "# Perform one-hot encoding on categorical features\n",
    "spaceship_dummies = pd.get_dummies(spaceship.drop(columns=['Transported']), drop_first=True)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = spaceship_dummies\n",
    "y = spaceship['Transported'].astype(int)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training features and transform them\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test features using the fitted scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Train and test sets created and features scaled.\")\n",
    "print(\"X_train_scaled shape:\", X_train_scaled.shape)\n",
    "print(\"X_test_scaled shape:\", X_test_scaled.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Selection** - now you will try to apply different ensemble methods in order to get a better model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bagging and Pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cleid\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py:865: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\n",
      "c:\\Users\\cleid\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py:871: RuntimeWarning: invalid value encountered in divide\n",
      "  oob_decision_function = predictions / predictions.sum(axis=1)[:, np.newaxis]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier Accuracy: 0.4945370902817711\n",
      "Bagging Classifier OOB Score: 0.4966925510497555\n",
      "Pasting Classifier Accuracy: 0.49511213341000576\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the base estimator (KNN Classifier)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Implement Bagging\n",
    "bagging_clf = BaggingClassifier(estimator=knn,\n",
    "                                n_estimators=5,  # Reduced number of estimators\n",
    "                                random_state=42,\n",
    "                                oob_score=True,\n",
    "                                n_jobs=-1)\n",
    "\n",
    "# Train the Bagging ensemble\n",
    "bagging_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_bagging = bagging_clf.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the Bagging ensemble\n",
    "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
    "print(\"Bagging Classifier Accuracy:\", accuracy_bagging)\n",
    "print(\"Bagging Classifier OOB Score:\", bagging_clf.oob_score_)\n",
    "\n",
    "# Implement Pasting\n",
    "pasting_clf = BaggingClassifier(estimator=knn,\n",
    "                                n_estimators=5,  # Reduced number of estimators\n",
    "                                random_state=42,\n",
    "                                bootstrap=False,\n",
    "                                n_jobs=-1)\n",
    "\n",
    "# Train the Pasting ensemble\n",
    "pasting_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_pasting = pasting_clf.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the Pasting ensemble\n",
    "accuracy_pasting = accuracy_score(y_test, y_pred_pasting)\n",
    "print(\"Pasting Classifier Accuracy:\", accuracy_pasting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Evaluation:\n",
      "Accuracy: 0.7878\n",
      "Precision: 0.7942\n",
      "Recall: 0.7825\n",
      "F1-score: 0.7883\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming X and y are your feature matrix and target vector\n",
    "# and they have been split into X_train, X_test, y_train, y_test\n",
    "# It's generally a good idea to work with unscaled data for tree-based models like Random Forests\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "# You can adjust hyperparameters like n_estimators, max_depth, etc.\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_rf = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the Random Forest model\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf)\n",
    "recall_rf = recall_score(y_test, y_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Random Forest Classifier Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy_rf:.4f}\")\n",
    "print(f\"Precision: {precision_rf:.4f}\")\n",
    "print(f\"Recall: {recall_rf:.4f}\")\n",
    "print(f\"F1-score: {f1_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Classifier Evaluation:\n",
      "Accuracy: 0.7832\n",
      "Precision: 0.7543\n",
      "Recall: 0.8462\n",
      "F1-score: 0.7976\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming X and y are your feature matrix and target vector\n",
    "# and they have been split into X_train, X_test, y_train, y_test\n",
    "# Tree-based models like Gradient Boosting generally work well with unscaled data\n",
    "\n",
    "# Initialize the Gradient Boosting Classifier\n",
    "# You can adjust hyperparameters like n_estimators, learning_rate, max_depth, etc.\n",
    "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Train the Gradient Boosting model\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_gb = gb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the Gradient Boosting model\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "precision_gb = precision_score(y_test, y_pred_gb)\n",
    "recall_gb = recall_score(y_test, y_pred_gb)\n",
    "f1_gb = f1_score(y_test, y_pred_gb)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Gradient Boosting Classifier Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy_gb:.4f}\")\n",
    "print(f\"Precision: {precision_gb:.4f}\")\n",
    "print(f\"Recall: {recall_gb:.4f}\")\n",
    "print(f\"F1-score: {f1_gb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adaptive Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Classifier Evaluation:\n",
      "Accuracy: 0.7688\n",
      "Precision: 0.7717\n",
      "Recall: 0.7699\n",
      "F1-score: 0.7708\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming X and y are your feature matrix and target vector\n",
    "# and they have been split into X_train, X_test, y_train, y_test\n",
    "# Tree-based models like AdaBoost generally work well with unscaled data\n",
    "\n",
    "# Initialize the base estimator (typically a Decision Tree with max_depth=1 - a stump)\n",
    "base_estimator = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# Initialize the AdaBoost Classifier\n",
    "# You can adjust hyperparameters like n_estimators, learning_rate, estimator, etc.\n",
    "adaboost_classifier = AdaBoostClassifier(estimator=base_estimator, n_estimators=50, learning_rate=1.0, random_state=42)\n",
    "\n",
    "# Train the AdaBoost model\n",
    "adaboost_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_ab = adaboost_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the AdaBoost model\n",
    "accuracy_ab = accuracy_score(y_test, y_pred_ab)\n",
    "precision_ab = precision_score(y_test, y_pred_ab)\n",
    "recall_ab = recall_score(y_test, y_pred_ab)\n",
    "f1_ab = f1_score(y_test, y_pred_ab)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"AdaBoost Classifier Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy_ab:.4f}\")\n",
    "print(f\"Precision: {precision_ab:.4f}\")\n",
    "print(f\"Recall: {recall_ab:.4f}\")\n",
    "print(f\"F1-score: {f1_ab:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model is the best and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Random Forest Classifier is clearly the best performing model among those you've evaluated so far.\n",
    "# Here's why:\n",
    "#\n",
    "# Significantly Higher Accuracy: The Random Forest achieved an accuracy of approximately 78.8%,\n",
    "# which is drastically higher than the accuracy of the Bagging (around 49.5%) and Pasting (around 49.5%)\n",
    "# classifiers using KNN as the base estimator. This means the Random Forest correctly predicted the\n",
    "# transportation status of a much larger proportion of the passengers in the test set.\n",
    "#\n",
    "# Balanced Precision and Recall: The Random Forest also shows a good balance between precision\n",
    "# (avoiding false positives) and recall (avoiding false negatives), as indicated by the relatively\n",
    "# close values of these metrics and a strong F1-score.\n",
    "#\n",
    "# Poor Performance of Bagging and Pasting with KNN: The Bagging and Pasting classifiers,\n",
    "# using KNN with n_neighbors=5 as the base estimator, are performing at a level close to random\n",
    "# guessing (an accuracy of 0.5 would be expected from a purely random binary classifier).\n",
    "# This suggests that either:\n",
    "#   - KNN with n_neighbors=5 is not a suitable base estimator for this problem, even within an ensemble.\n",
    "#   - The number of estimators (set to 5 in your provided Bagging/Pasting code) is too low to provide\n",
    "#     any significant benefit from the ensemble.\n",
    "#   - There might be other issues with how KNN is interacting with the high-dimensional, sparse data\n",
    "#     after one-hot encoding, even after scaling.\n",
    "#\n",
    "# In conclusion, based on the metrics you've shared, the Random Forest Classifier is the superior\n",
    "# model due to its much higher accuracy and balanced precision and recall compared to the Bagging\n",
    "# and Pasting ensembles using KNN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
